<files>
This section contains the contents of the repository's files.

<file path="src/core/services/utils/__init__.py">
from .collection_name import get_collection_name
from .embedding_service import Embedding, EmbeddingService
from .explainer_service import ExplainerService
__all__ = [
    "EmbeddingService",
    "Embedding",
    "ExplainerService",
    "get_collection_name",
]
</file>

<file path="src/core/services/utils/collection_name.py">
import hashlib
from pathlib import Path
PREFIX = "code_chunks_"
def get_collection_name(path: Path) -> str:
    path_hash = hashlib.md5(str(path.absolute()).encode("utf-8")).hexdigest()
    return f"code_chunks_{path_hash[:8]}"
</file>

<file path="src/core/services/utils/embedding_service.py">
import itertools
from openai import AsyncOpenAI, RateLimitError
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)
Embedding = list[float]
class EmbeddingService:
    def __init__(self, base_url: str, api_key: str) -> None:
        self.openai = AsyncOpenAI(base_url=base_url, api_key=api_key)
    @retry(
        wait=wait_exponential(min=5, max=20),
        stop=stop_after_attempt(3),
        retry=retry_if_exception_type(RateLimitError),
    )
    async def generate_embedding(self, query: str, model: str) -> Embedding:
        response = await self.openai.embeddings.create(input=query, model=model)
        return response.data[0].embedding
    async def generate_embeddings(
        self, queries: list[str], model: str, batch_size: int = 32
    ) -> list[Embedding]:
        all_embeddings: list[Embedding] = []
        for query_batch in itertools.batched(queries, batch_size):
            response = await self.openai.embeddings.create(
                input=query_batch, model=model
            )
            embedding_batch = [d.embedding for d in response.data]
            all_embeddings.extend(embedding_batch)
        return all_embeddings
</file>

<file path="src/core/services/utils/explainer_service.py">
import asyncio
import itertools
from openai import AsyncOpenAI, RateLimitError
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)
class ExplainerService:
    _SYSTEM_PROMPT: str = """
    You will receive only source code as input. Return exactly one concise English sentence that captures the codeâ€™s purpose and intent - no code, no labels, no examples, no extra sentences.
    """
    _DEFAULT_EXPLANATION: str = "unknown"
    def __init__(self, base_url: str, api_key: str, parallelism: int = 1) -> None:
        self.openai = AsyncOpenAI(base_url=base_url, api_key=api_key)
        self.parallelism = parallelism
    @retry(
        wait=wait_exponential(min=5, max=20),
        stop=stop_after_attempt(3),
        retry=retry_if_exception_type(RateLimitError),
    )
    async def _get_explanation(self, code_chunk: str, model: str) -> str:
        response = await self.openai.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": ExplainerService._SYSTEM_PROMPT},
                {"role": "user", "content": code_chunk},
            ],
        )
        explanation = response.choices[0].message.content
        if explanation is None:
            return ExplainerService._DEFAULT_EXPLANATION
        return explanation
    async def _get_sync_explanations(
        self, code_chunks: list[str], model: str
    ) -> list[str]:
        explanations: list[str] = []
        for code in code_chunks:
            explanations.append(await self._get_explanation(code, model))
        return explanations
    async def _get_parallel_explanations(
        self, code_chunks: list[str], model: str
    ) -> list[str]:
        batches = itertools.batched(code_chunks, self.parallelism)
        explanations: list[str] = []
        for batch in batches:
            tasks = [
                asyncio.create_task(self._get_explanation(chunk, model))
                for chunk in batch
            ]
            results = await asyncio.gather(*tasks)
            explanations.extend(results)
        return explanations
    async def get_explanations(self, code_chunks: list[str], model: str) -> list[str]:
        if self.parallelism == 1:
            return await self._get_sync_explanations(code_chunks, model)
        return await self._get_parallel_explanations(code_chunks, model)
</file>

<file path="src/core/services/__init__.py">
from .indexing_service import EmbeddingConfig, ExplainerConfig, IndexingService
from .search_service import SearchResult, SearchService
from .utils import EmbeddingService, ExplainerService
__all__ = [
    "IndexingService",
    "EmbeddingConfig",
    "ExplainerConfig",
    "ExplainerService",
    "EmbeddingService",
    "SearchService",
    "SearchResult",
]
</file>

<file path="src/core/services/constants.py">
TEXT_EMBEDDING_MODEL = "Qdrant/bm25"
CODE_INDEX = "code"
EXPLANATION_INDEX = "explanation"
TEXT_INDEX = "bm25"
</file>

<file path="src/core/services/indexing_service.py">
import itertools
import uuid
from dataclasses import dataclass
from pathlib import Path
from loguru import logger
from qdrant_client import AsyncQdrantClient, models
from qdrant_client.models import FieldCondition, Filter, MatchValue
from core.splitters import CodeChunk, Splitter
from core.sync import FileSynchronizer
from .constants import CODE_INDEX, EXPLANATION_INDEX, TEXT_EMBEDDING_MODEL, TEXT_INDEX
from .utils import Embedding, EmbeddingService, ExplainerService, get_collection_name
ITER_BATCH_SIZE = 128
@dataclass
class EmbeddingConfig:
    service: EmbeddingService
    model: str
    size: int
    batch_size: int = 32
@dataclass
class ExplainerConfig:
    service: ExplainerService
    model: str
    embedding: EmbeddingConfig
@dataclass
class Explanations:
    text: list[str]
    embeddings: list[Embedding]
class IndexingService:
    def __init__(
        self,
        client: AsyncQdrantClient,
        file_syncrhonizer: FileSynchronizer,
    ):
        self.client = client
        self.synchronizer = file_syncrhonizer
    async def delete(self, codebase_path: Path) -> None:
        codebase_path = codebase_path.expanduser().absolute().resolve()
        collection_name = get_collection_name(codebase_path)
        collection_exists = await self.client.collection_exists(collection_name)
        if collection_exists:
            await self.client.delete_collection(collection_name)
        await self.synchronizer.delete_snapshot(codebase_path)
    async def index(
        self,
        codebase_path: Path,
        splitter: Splitter,
        embedding: EmbeddingConfig,
        explainer: ExplainerConfig | None = None,
        force_reindex: bool = False,
    ) -> None:
        """Index a codebase, automatically handling initial indexing or incremental reindexing.
        Args:
            codebase_path: Path to the codebase to index
            force_reindex: Whether to force a complete reindexing
        Returns:
            IndexingStats with information about the indexing operation
        """
        codebase_path = codebase_path.expanduser().absolute().resolve()
        if not codebase_path.exists():
            raise RuntimeError("Invalid path")
        logger.debug("Starting indexing for codebase: {}", codebase_path)
        await self._prepare_collection(
            codebase_path,
            embedding.size,
            explainer.embedding.size if explainer is not None else None,
            force_reindex,
        )
        collection_name = get_collection_name(codebase_path)
        results = await self.synchronizer.check_for_changes(codebase_path)
        if results.num_changes == 0:
            logger.debug("No changes found")
            return
        await self._delete_file_chunks(collection_name, results.to_remove)
        chunks = await self._get_chunks(codebase_path, results.to_add, splitter)
        for chunk_batch in itertools.batched(chunks, ITER_BATCH_SIZE):
            contents = [c.content for c in chunk_batch]
            code_embeddings = await embedding.service.generate_embeddings(
                contents, embedding.model, embedding.batch_size
            )
            explanations: Explanations | None = None
            if explainer is not None:
                explanation_texts = await explainer.service.get_explanations(
                    contents, explainer.model
                )
                explanation_embeddings = (
                    await explainer.embedding.service.generate_embeddings(
                        explanation_texts, explainer.embedding.model
                    )
                )
                explanations = Explanations(explanation_texts, explanation_embeddings)
            points = await self._get_points(
                list(chunk_batch), code_embeddings, explanations
            )
            await self.client.upsert(collection_name, points)
    async def _get_points(
        self,
        chunks: list[CodeChunk],
        embeddings: list[Embedding],
        explanations: Explanations | None = None,
    ) -> list[models.PointStruct]:
        if explanations is None:
            return [
                models.PointStruct(
                    id=uuid.uuid4().hex,
                    vector={
                        "code": emb,
                        "bm25": models.Document(
                            text=chunk.content, model=TEXT_EMBEDDING_MODEL
                        ),
                    },
                    payload={
                        "content": chunk.content,
                        "relative_path": chunk.file_path,
                        "start_line": chunk.start_line,
                        "end_line": chunk.end_line,
                        "file_extension": chunk.language,
                    },
                )
                for chunk, emb in zip(chunks, embeddings)
            ]
        return [
            models.PointStruct(
                id=uuid.uuid4().hex,
                vector={
                    "code": code_emb,
                    "explanation": exp_emb,
                    "bm25": models.Document(
                        text=chunk.content, model=TEXT_EMBEDDING_MODEL
                    ),
                },
                payload={
                    "content": chunk.content,
                    "relative_path": chunk.file_path,
                    "start_line": chunk.start_line,
                    "end_line": chunk.end_line,
                    "file_extension": chunk.language,
                    "explanation": exp_text,
                },
            )
            for chunk, code_emb, exp_text, exp_emb in zip(
                chunks, embeddings, explanations.text, explanations.embeddings
            )
        ]
    async def _get_chunks(
        self, codebase_path: Path, files: list[str], splitter: Splitter
    ) -> list[CodeChunk]:
        all_chunks: list[CodeChunk] = []
        for file in files:
            file_path = codebase_path / file
            content = file_path.read_text(encoding="utf-8")
            chunks = await splitter.split(content, file_path)
            all_chunks.extend(chunks)
        return all_chunks
    async def _prepare_collection(
        self,
        codebase_path: Path,
        code_size: int,
        explanation_size: int | None = None,
        force_reindex: bool = False,
    ) -> None:
        collection_name = get_collection_name(codebase_path)
        collection_exists = await self.client.collection_exists(collection_name)
        if collection_exists and not force_reindex:
            logger.debug(
                "Collection {} already exists, skipping creation", collection_name
            )
            return
        if collection_exists and force_reindex:
            logger.debug(
                "Dropping existing collection {} for force reindex", collection_name
            )
            await self.delete(codebase_path)
        await self._create_collection(collection_name, code_size, explanation_size)
        logger.debug("Collection {} created successfully", collection_name)
    async def _create_collection(
        self, collection_name: str, code_size: int, explanation_size: int | None = None
    ) -> None:
        dense_vectors: dict[str, models.VectorParams] = {
            CODE_INDEX: models.VectorParams(
                size=code_size,
                distance=models.Distance.COSINE,
            )
        }
        if explanation_size is not None:
            dense_vectors[EXPLANATION_INDEX] = models.VectorParams(
                size=explanation_size,
                distance=models.Distance.COSINE,
            )
        await self.client.create_collection(
            collection_name=collection_name,
            vectors_config=dense_vectors,
            sparse_vectors_config={
                TEXT_INDEX: models.SparseVectorParams(modifier=models.Modifier.IDF)
            },
        )
    async def _get_embeddings(
        self, chunks: list[CodeChunk], config: EmbeddingConfig
    ) -> list[Embedding]:
        all_embeddings: list[Embedding] = []
        for chunk_batch in itertools.batched(chunks, config.batch_size):
            contents = [c.content for c in chunk_batch]
            embeddings = await config.service.generate_embeddings(
                contents, config.model
            )
            all_embeddings.extend(embeddings)
        return all_embeddings
    async def _delete_file_chunks(
        self, collection_name: str, file_paths: list[str]
    ) -> None:
        for path in file_paths:
            filter_condition = Filter(
                must=[FieldCondition(key="relative_path", match=MatchValue(value=path))]
            )
            await self.client.delete(collection_name, filter_condition)
</file>

<file path="src/core/services/search_service.py">
from dataclasses import dataclass
from pathlib import Path
from loguru import logger
from qdrant_client import AsyncQdrantClient, models
from .constants import CODE_INDEX, EXPLANATION_INDEX, TEXT_EMBEDDING_MODEL, TEXT_INDEX
from .utils import EmbeddingService, get_collection_name
@dataclass
class SearchResult:
    content: str
    explanation: str | None
    relative_path: str
    start_line: int
    end_line: int
    language: str
    score: float
class SearchService:
    def __init__(
        self,
        client: AsyncQdrantClient,
        embedding_service: EmbeddingService,
        code_model: str,
        explanation_model: str,
    ):
        self.client = client
        self.embedding_service = embedding_service
        self.code_model = code_model
        self.explanation_model = explanation_model
    async def _preform_search(
        self,
        collection_name: str,
        query_text: str,
        has_explanations: bool = False,
        limit: int = 10,
        threshold: float = 0.0,
    ) -> list[SearchResult]:
        prefetch = [
            models.Prefetch(
                query=await self.embedding_service.generate_embedding(
                    query_text, self.code_model
                ),
                using=CODE_INDEX,
                limit=limit * 2,
            ),
            models.Prefetch(
                query=models.Document(text=query_text, model=TEXT_EMBEDDING_MODEL),
                using=TEXT_INDEX,
                limit=limit * 2,
            ),
        ]
        if has_explanations:
            prefetch.append(
                models.Prefetch(
                    query=await self.embedding_service.generate_embedding(
                        query_text, self.explanation_model
                    ),
                    using=EXPLANATION_INDEX,
                    limit=limit * 2,
                ),
            )
        search_result = await self.client.query_points(
            collection_name=collection_name,
            prefetch=prefetch,
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=limit,
            score_threshold=threshold,
        )
        results = []
        for point in search_result.points:
            payload = point.payload or {}
            results.append(
                SearchResult(
                    content=payload.get("content", ""),
                    explanation=payload.get("explanation", None),
                    relative_path=payload.get("relative_path", ""),
                    start_line=payload.get("start_line", 0),
                    end_line=payload.get("end_line", 0),
                    language=payload.get("metadata", {}).get("language", "unknown"),
                    score=point.score,
                )
            )
        logger.debug("Found {} results for text query", len(results))
        return results
    async def search(
        self,
        codebase_path: Path,
        query: str,
        has_explanations: bool = False,
        top_k: int = 5,
        threshold: float = 0.5,
    ) -> list[SearchResult]:
        """Search indexed code semantically.
        Args:
            codebase_path: Path to the codebase to search in
            query: Search query (must not be empty)
            has_explanations: Whether codebase was indexed with explanation vector
            top_k: Number of results to return (1-50)
            threshold: Similarity threshold (0.0-1.0)
        Returns:
            List of search results
        Raises:
            CollectionNotIndexedError: If the codebase is not indexed
            ValueError: If query is empty or parameters are invalid
        """
        if not query or query.strip() == "":
            raise ValueError("Search query cannot be empty")
        if not (1 <= top_k <= 50):
            raise ValueError("top_k must be between 1 and 50")
        if not (0.0 <= threshold <= 1.0):
            raise ValueError("threshold must be between 0.0 and 1.0")
        codebase_path = codebase_path.expanduser().absolute().resolve()
        collection_name = get_collection_name(codebase_path)
        logger.debug("Searching in codebase: {}", codebase_path)
        if not await self.client.collection_exists(collection_name):
            logger.warning(
                "Collection '{}' does not exist for codebase '{}'",
                collection_name,
                codebase_path,
            )
            raise RuntimeError(
                f"Collection not indexed {collection_name}, path: {codebase_path}"
            )
        logger.debug("Searching with query: '{}'", query)
        results = await self._preform_search(
            collection_name, query, has_explanations, top_k, threshold
        )
        logger.debug("Found {} relevant results", len(results))
        return results
</file>

<file path="src/core/splitters/__init__.py">
from .base import Splitter
from .tree_sitter import TreeSitterSplitter
from .types import CodeChunk
from .utils import SUPPORTED_EXTENSIONS, is_file_supported
__all__ = [
    "TreeSitterSplitter",
    "Splitter",
    "CodeChunk",
    "SUPPORTED_EXTENSIONS",
    "is_file_supported",
]
</file>

<file path="src/core/splitters/base.py">
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Protocol
from .types import CodeChunk
class Splitter(Protocol):
    """Protocol for code splitters."""
    async def split(self, code: str, file_path: Path) -> list[CodeChunk]:
        """Split code into chunks.
        Args:
            code: Code content to split
            language: Programming language
            file_path: Path to the file
        Returns:
            List of code chunks
        """
        ...
    def set_chunk_size(self, chunk_size: int) -> None:
        """Set chunk size.
        Args:
            chunk_size: Maximum chunk size in characters
        """
        ...
    def set_chunk_overlap(self, chunk_overlap: int) -> None:
        """Set chunk overlap.
        Args:
            chunk_overlap: Overlap size in characters
        """
        ...
class BaseSplitter(ABC):
    """Abstract base class for splitters."""
    def __init__(self, chunk_size: int = 2500, chunk_overlap: int = 300) -> None:
        """Initialize splitter.
        Args:
            chunk_size: Maximum chunk size in characters
            chunk_overlap: Overlap size in characters
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    @abstractmethod
    async def split(self, code: str, file_path: Path) -> list[CodeChunk]:
        """Split code into chunks.
        Args:
            code: Code content to split
            language: Programming language
            file_path: Path to the file
        Returns:
            List of code chunks
        """
        ...
    def set_chunk_size(self, chunk_size: int) -> None:
        """Set chunk size.
        Args:
            chunk_size: Maximum chunk size in characters
        """
        self.chunk_size = chunk_size
    def set_chunk_overlap(self, chunk_overlap: int) -> None:
        """Set chunk overlap.
        Args:
            chunk_overlap: Overlap size in characters
        """
        self.chunk_overlap = chunk_overlap
</file>

<file path="src/core/splitters/tree_sitter.py">
from pathlib import Path
from loguru import logger
from tree_sitter import Node, Parser
from tree_sitter_language_pack import SupportedLanguage, get_parser
from .base import BaseSplitter
from .types import CodeChunk
from .utils import LANGUAGE_EXTENSIONS, SPLITTABLE_NODE_TYPES
class TreeSitterSplitter(BaseSplitter):
    """Tree-sitter based code splitter with fallback to text splitting."""
    def __init__(self, chunk_size: int = 2500, chunk_overlap: int = 300) -> None:
        """Initialize tree-sitter splitter.
        Args:
            chunk_size: Maximum chunk size in characters
            chunk_overlap: Overlap size in characters
        """
        super().__init__(chunk_size, chunk_overlap)
        self._parsers: dict[SupportedLanguage, Parser] = {}
    def _get_parser(self, lang: SupportedLanguage) -> Parser | None:
        if lang in self._parsers:
            return self._parsers[lang]
        try:
            parser = get_parser(lang)
            self._parsers[lang] = parser
            logger.debug("Loaded tree-sitter parser for {}", lang)
            return parser
        except Exception as e:
            logger.warning("Failed to load tree-sitter parser for %s: %s", lang, e)
            return None
    async def split(self, code: str, file_path: Path) -> list[CodeChunk]:
        """Split code into chunks using tree-sitter.
        Args:
            code: Code content to split
            language: Programming language
            file_path: Path to the file
        Returns:
            List of code chunks
        """
        lang = LANGUAGE_EXTENSIONS.get(file_path.suffix)
        if lang is None:
            logger.debug("File type not supported by Tree Sitter %s", file_path.suffix)
            return await self._fallback_text_split(code, file_path)
        parser = self._get_parser(lang)
        if not parser:
            return await self._fallback_text_split(code, file_path)
        try:
            tree = parser.parse(bytes(code, "utf-8"))
            if not tree.root_node:
                logger.warning("Failed to parse AST for %s", file_path.name)
                return await self._fallback_text_split(code, file_path)
            chunks = self._extract_chunks(tree.root_node, lang, code, file_path)
            refined_chunks = await self._refine_chunks(chunks)
            return refined_chunks
        except Exception as e:
            logger.warning("Tree Sitter failed for %s %s", file_path.name, e)
            return await self._fallback_text_split(code, file_path)
    def _extract_chunks(
        self,
        node: Node,
        lang: SupportedLanguage,
        code: str,
        file_path: Path,
    ) -> list[CodeChunk]:
        chunks: list[CodeChunk] = []
        splittable_types = SPLITTABLE_NODE_TYPES.get(lang)
        if splittable_types is None or len(splittable_types) == 0:
            raise RuntimeError("Invalid splittable types for %s", lang)
        def traverse(current_node: Node) -> None:
            """Recursively traverse AST nodes."""
            if current_node.type in splittable_types:
                start_line = current_node.start_point[0] + 1
                end_line = current_node.end_point[0] + 1
                start_byte = current_node.start_byte
                end_byte = current_node.end_byte
                node_text = code.encode("utf-8")[start_byte:end_byte].decode("utf-8")
                if node_text.strip():
                    chunks.append(
                        CodeChunk(
                            content=node_text,
                            start_line=start_line,
                            end_line=end_line,
                            language=lang,
                            file_path=str(file_path),
                        )
                    )
            for child in current_node.children:
                traverse(child)
        traverse(node)
        if not chunks:
            lines = code.split("\n")
            chunks.append(
                CodeChunk(
                    content=code,
                    start_line=1,
                    end_line=len(lines),
                    language=lang,
                    file_path=str(file_path),
                )
            )
        return chunks
    async def _refine_chunks(
        self,
        chunks: list[CodeChunk],
    ) -> list[CodeChunk]:
        refined_chunks: list[CodeChunk] = []
        for chunk in chunks:
            if len(chunk.content) <= self.chunk_size:
                refined_chunks.append(chunk)
            else:
                sub_chunks = self._split_large_chunk(chunk)
                refined_chunks.extend(sub_chunks)
        return self._add_overlap(refined_chunks)
    def _split_large_chunk(self, chunk: CodeChunk) -> list[CodeChunk]:
        lines = chunk.content.split("\n")
        sub_chunks: list[CodeChunk] = []
        current_chunk = ""
        current_start_line = chunk.start_line
        current_line_count = 0
        for i, line in enumerate(lines):
            line_with_newline = line + "\n" if i < len(lines) - 1 else line
            if (
                len(current_chunk) + len(line_with_newline) > self.chunk_size
                and current_chunk.strip()
            ):
                sub_chunks.append(
                    CodeChunk(
                        content=current_chunk.strip(),
                        start_line=current_start_line,
                        end_line=current_start_line + current_line_count - 1,
                        language=chunk.language,
                        file_path=chunk.file_path,
                    )
                )
                current_chunk = line_with_newline
                current_start_line = chunk.start_line + i
                current_line_count = 1
            else:
                current_chunk += line_with_newline
                current_line_count += 1
        if current_chunk.strip():
            sub_chunks.append(
                CodeChunk(
                    content=current_chunk.strip(),
                    start_line=current_start_line,
                    end_line=current_start_line + current_line_count - 1,
                    language=chunk.language,
                    file_path=chunk.file_path,
                )
            )
        return sub_chunks
    def _add_overlap(self, chunks: list[CodeChunk]) -> list[CodeChunk]:
        if len(chunks) <= 1 or self.chunk_overlap <= 0:
            return chunks
        overlapped_chunks: list[CodeChunk] = []
        for i, chunk in enumerate(chunks):
            content = chunk.content
            metadata = chunk
            if i > 0 and self.chunk_overlap > 0:
                prev_chunk = chunks[i - 1]
                overlap_text = prev_chunk.content[-self.chunk_overlap :]
                content = overlap_text + "\n" + content
                metadata.start_line = max(
                    1, metadata.start_line - self._get_line_count(overlap_text)
                )
            overlapped_chunks.append(
                CodeChunk(
                    content=content,
                    start_line=metadata.start_line,
                    end_line=metadata.end_line,
                    language=metadata.language,
                    file_path=metadata.file_path,
                )
            )
        return overlapped_chunks
    def _get_line_count(self, text: str) -> int:
        return len(text.split("\n"))
    async def _fallback_text_split(self, code: str, file_path: Path) -> list[CodeChunk]:
        lines = code.split("\n")
        chunks: list[CodeChunk] = []
        current_chunk = ""
        current_start_line = 1
        lang = LANGUAGE_EXTENSIONS.get(file_path.suffix)
        if lang is None:
            return []
        for i, line in enumerate(lines):
            line_with_newline = line + "\n"
            if (
                len(current_chunk) + len(line_with_newline) > self.chunk_size
                and current_chunk.strip()
            ):
                chunks.append(
                    CodeChunk(
                        content=current_chunk.strip(),
                        start_line=current_start_line,
                        end_line=i,
                        language=lang,
                        file_path=str(file_path),
                    )
                )
                current_chunk = line_with_newline
                current_start_line = i + 1
            else:
                current_chunk += line_with_newline
        if current_chunk.strip():
            chunks.append(
                CodeChunk(
                    content=current_chunk.strip(),
                    start_line=current_start_line,
                    end_line=len(lines),
                    language=lang,
                    file_path=str(file_path),
                )
            )
        return chunks
</file>

<file path="src/core/splitters/types.py">
from dataclasses import dataclass
from tree_sitter_language_pack import SupportedLanguage
@dataclass
class CodeChunk:
    content: str
    start_line: int
    end_line: int
    language: SupportedLanguage
    file_path: str
</file>

<file path="src/core/splitters/utils.py">
from pathlib import Path
from tree_sitter_language_pack import SupportedLanguage
LANGUAGE_EXTENSIONS: dict[str, SupportedLanguage] = {
    ".ts": "typescript",
    ".tsx": "typescript",
    ".js": "javascript",
    ".jsx": "javascript",
    ".py": "python",
    ".java": "java",
    ".cpp": "cpp",
    ".c": "c",
    ".h": "c",
    ".hpp": "cpp",
    ".cs": "csharp",
    ".go": "go",
    ".rs": "rust",
    ".php": "php",
    ".rb": "ruby",
    ".swift": "swift",
    ".kt": "kotlin",
    ".scala": "scala",
}
SUPPORTED_EXTENSIONS = set(LANGUAGE_EXTENSIONS.keys())
SPLITTABLE_NODE_TYPES: dict[SupportedLanguage, list[str]] = {
    "javascript": [
        "function_declaration",
        "arrow_function",
        "class_declaration",
        "method_definition",
        "export_statement",
    ],
    "typescript": [
        "function_declaration",
        "arrow_function",
        "class_declaration",
        "method_definition",
        "export_statement",
        "interface_declaration",
        "type_alias_declaration",
    ],
    "python": [
        "function_definition",
        "class_definition",
        "decorated_definition",
        "async_function_definition",
    ],
    "java": [
        "method_declaration",
        "class_declaration",
        "interface_declaration",
        "constructor_declaration",
    ],
    "cpp": [
        "function_definition",
        "class_specifier",
        "namespace_definition",
        "declaration",
    ],
    "go": [
        "function_declaration",
        "method_declaration",
        "type_declaration",
        "var_declaration",
        "const_declaration",
    ],
    "rust": [
        "function_item",
        "impl_item",
        "struct_item",
        "enum_item",
        "trait_item",
        "mod_item",
    ],
    "csharp": [
        "method_declaration",
        "class_declaration",
        "interface_declaration",
        "struct_declaration",
        "enum_declaration",
    ],
    "scala": [
        "method_declaration",
        "class_declaration",
        "interface_declaration",
        "constructor_declaration",
    ],
}
def is_file_supported(path: Path) -> bool:
    if path.is_file():
        return path.suffix is SUPPORTED_EXTENSIONS
    return True
</file>

<file path="src/core/sync/__init__.py">
from .files import FileSynchronizer
__all__ = ["FileSynchronizer"]
</file>

<file path="src/core/sync/comparator.py">
from pathlib import Path
from .scanner import _sha256_of_file
from .types import DetectedChanges, FileRecord
async def compare_snapshot_to_current(
    root: Path,
    old_files: dict[str, FileRecord],
    current_meta: dict[str, tuple[int, float, int | None]],
) -> DetectedChanges:
    """Compare old snapshot vs current metadata. Conservative: never miss a modification.
    Returns DetectedChanges with lists of relative paths.
    """
    old_paths: set[str] = set(old_files.keys())
    new_paths: set[str] = set(current_meta.keys())
    added = sorted(list(new_paths - old_paths))
    removed = sorted(list(old_paths - new_paths))
    common = sorted(list(old_paths & new_paths))
    modified: list[str] = []
    # For common paths: if metadata changed -> compute hash and compare
    for p in common:
        size, mtime, inode = current_meta[p]
        old = old_files[p]
        if old.size == size and old.mtime == mtime and old.inode == inode:
            continue
        # metadata changed; compute current hash and compare
        curr_hash = _sha256_of_file(root / p)
        if curr_hash != old.hash:
            modified.append(p)
        # else: metadata changed but content same -> treat as unchanged
    # Attempt rename/move detection for added/removed pairs
    added_set: set[str] = set(added)
    removed_set: set[str] = set(removed)
    # Build old inode -> path and old hash -> path maps
    old_inode_map: dict[int, str] = {}
    old_hash_map: dict[str, str] = {}
    for path, rec in old_files.items():
        if rec.inode is not None:
            old_inode_map[rec.inode] = path
        old_hash_map[rec.hash] = path
    # 1) inode-based detection (verify content if metadata differs)
    to_remove_added: set[str] = set()
    to_remove_removed: set[str] = set()
    for new_p in list(added_set):
        _, _, new_inode = current_meta[new_p]
        if new_inode is None:
            continue
        old_p = old_inode_map.get(new_inode)
        if not old_p:
            continue
        # we found a candidate; must ensure content same to avoid missing modifications
        old_rec = old_files[old_p]
        curr_hash = _sha256_of_file(root / new_p)
        if curr_hash == old_rec.hash:
            to_remove_added.add(new_p)
            to_remove_removed.add(old_p)
        else:
            # inode same but content changed -> treat as modification of old path if the path still present?
            # Conservative approach: mark new_p as modified (appeared changed) and keep removed old_p as removed
            modified.append(new_p)
    added_set -= to_remove_added
    removed_set -= to_remove_removed
    # 2) hash-based detection (for remaining added)
    removed_hashes = {old_files[p].hash: p for p in removed_set}
    to_remove_added = set()
    to_remove_removed = set()
    for new_p in list(added_set):
        curr_hash = _sha256_of_file(root / new_p)
        old_p = removed_hashes.get(curr_hash)
        if old_p:
            to_remove_added.add(new_p)
            to_remove_removed.add(old_p)
    added_set -= to_remove_added
    removed_set -= to_remove_removed
    # Final lists
    final_added = sorted(list(added_set))
    final_removed = sorted(list(removed_set))
    # deduplicate modified
    modified = sorted(list(dict.fromkeys(modified)))
    return DetectedChanges(added=final_added, modified=modified, removed=final_removed)
</file>

<file path="src/core/sync/files.py">
from pathlib import Path
from loguru import logger
from .comparator import compare_snapshot_to_current
from .io import load_snapshot, snapshot_path_for, write_snapshot
from .scanner import build_snapshot_records, scan_and_hash_all, scan_metadata
from .types import DetectedChanges
from .util import DEFAULT_IGNORE_PATTERNS
class FileSynchronizer:
    def __init__(
        self,
        snapshots_dir: Path | None = None,
        ignore_patterns: list[str] | None = None,
    ) -> None:
        self.snapshots_dir = (
            (snapshots_dir or (Path.home() / ".code-context" / "snapshots"))
            .expanduser()
            .resolve()
        )
        self.snapshots_dir.mkdir(parents=True, exist_ok=True)
        self.ignore_patterns = frozenset(
            (ignore_patterns or []) + DEFAULT_IGNORE_PATTERNS
        )
    async def check_for_changes(self, codebase_path: Path) -> DetectedChanges:
        codebase_path = codebase_path.expanduser().resolve()
        snapshot_path = snapshot_path_for(codebase_path, self.snapshots_dir)
        # If no snapshot exists, create initial snapshot (hash every file) and
        # report every file as added.
        if not snapshot_path.exists():
            initial = await scan_and_hash_all(codebase_path, self.ignore_patterns)
            write_snapshot(snapshot_path, initial)
            return DetectedChanges(added=sorted(initial.keys()))
        # Load previous snapshot from disk (synchronizer is ephemeral; always load)
        old_files = load_snapshot(snapshot_path)
        # Fast metadata scan
        current_meta = await scan_metadata(codebase_path, self.ignore_patterns)
        # Compare
        changes = await compare_snapshot_to_current(
            codebase_path, old_files, current_meta
        )
        # If changes occurred, rebuild snapshot records (reusing hashes where unchanged)
        if changes.added or changes.modified or changes.removed:
            new_records = await build_snapshot_records(
                codebase_path, current_meta, old_files
            )
            write_snapshot(snapshot_path, new_records)
        return changes
    async def delete_snapshot(self, codebase_path: Path) -> None:
        """
        Delete the snapshot file for the given codebase path (if present).
        Raises:
            OSError: if filesystem unlink fails for reasons other than file not existing.
        """
        snapshot_path = snapshot_path_for(codebase_path, self.snapshots_dir)
        try:
            if snapshot_path.exists():
                snapshot_path.unlink()
                logger.debug("Deleted snapshot file: {}", snapshot_path)
            else:
                logger.debug(
                    "Snapshot file not found (already deleted): {}", snapshot_path
                )
        except OSError as exc:
            logger.error("Failed to delete snapshot file {}: {}", snapshot_path, exc)
            raise
</file>

<file path="src/core/sync/io.py">
import hashlib
import json
from pathlib import Path
from .types import FileRecord
SNAPSHOT_VERSION = 1
def snapshot_path_for(codebase_path: Path, snapshots_dir: Path) -> Path:
    resolved = str(codebase_path.resolve())
    name = hashlib.md5(resolved.encode("utf-8")).hexdigest()
    return snapshots_dir / f"{name}.json"
def write_snapshot(path: Path, files: dict[str, FileRecord]) -> None:
    payload = {
        "version": SNAPSHOT_VERSION,
        "files": {p: r.to_dict() for p, r in files.items()},
    }
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
def load_snapshot(path: Path) -> dict[str, FileRecord]:
    try:
        raw = path.read_text(encoding="utf-8")
        data = json.loads(raw)
        if int(data.get("version", 0)) != SNAPSHOT_VERSION:
            return {}
        return {p: FileRecord.from_dict(d) for p, d in data.get("files", {}).items()}
    except Exception:
        return {}
</file>

<file path="src/core/sync/scanner.py">
import fnmatch
import hashlib
import os
from pathlib import Path
from core.splitters import SUPPORTED_EXTENSIONS
from .types import FileRecord
def _sha256_of_file(path: Path, chunk_size: int = 65536) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()
def _should_ignore(
    relative_path: Path,
    is_directory: bool = False,
    ignore_patterns: frozenset[str] = frozenset(),
) -> bool:
    if any(part.startswith(".") for part in relative_path.parts):
        return True
    if not is_directory and relative_path.suffix not in SUPPORTED_EXTENSIONS:
        return True
    if not ignore_patterns:
        return False
    normalized = str(relative_path).replace(os.sep, "/").strip("/")
    if not normalized:
        return False
    def _match(file_path: str, pattern: str, is_dir: bool) -> bool:
        clean = pattern.strip("/")
        if not clean:
            return False
        if pattern.endswith("/"):
            if not is_dir:
                return False
            for part in file_path.split("/"):
                if fnmatch.fnmatchcase(part, clean):
                    return True
            return False
        if "/" in clean:
            return fnmatch.fnmatchcase(file_path, clean)
        basename = Path(file_path).name
        return fnmatch.fnmatchcase(basename, clean)
    for pat in ignore_patterns:
        if _match(normalized, pat, is_directory):
            return True
    return False
async def scan_metadata(
    root: Path, ignore_patterns: frozenset[str] = frozenset()
) -> dict[str, tuple[int, float, int | None]]:
    """
    Fast metadata-only scan: returns mapping relative_path -> (size, mtime, inode)
    """
    root = root.resolve()
    result: dict[str, tuple[int, float, int | None]] = {}
    stack: list[Path] = [root]
    while stack:
        directory = stack.pop()
        try:
            with os.scandir(directory) as it:
                for entry in it:
                    try:
                        rel = Path(entry.path).relative_to(root)
                    except Exception:
                        rel = Path(str(entry.path).replace(str(root) + os.sep, "", 1))
                    if _should_ignore(rel, entry.is_dir(), ignore_patterns):
                        continue
                    if entry.is_dir(follow_symlinks=False):
                        stack.append(Path(entry.path))
                    elif entry.is_file(follow_symlinks=False):
                        try:
                            st = entry.stat(follow_symlinks=False)
                            inode = getattr(st, "st_ino", None)
                            result[str(rel)] = (
                                st.st_size,
                                st.st_mtime,
                                int(inode) if inode is not None else None,
                            )
                        except Exception:
                            continue
        except Exception:
            continue
    return result
async def scan_and_hash_all(root: Path, should_ignore) -> dict[str, FileRecord]:
    meta = await scan_metadata(root, should_ignore)
    records: dict[str, FileRecord] = {}
    for rel, (size, mtime, inode) in meta.items():
        p = root / rel
        try:
            h = _sha256_of_file(p)
        except Exception:
            continue
        records[rel] = FileRecord(size=size, mtime=mtime, inode=inode, hash=h)
    return records
async def build_snapshot_records(
    root: Path,
    meta_map: dict[str, tuple[int, float, int | None]],
    prev_snapshot: dict[str, FileRecord],
) -> dict[str, FileRecord]:
    res: dict[str, FileRecord] = {}
    for rel, (size, mtime, inode) in meta_map.items():
        prev = prev_snapshot.get(rel)
        if prev and prev.size == size and prev.mtime == mtime and prev.inode == inode:
            res[rel] = prev
            continue
        p = root / rel
        try:
            h = _sha256_of_file(p)
        except Exception:
            continue
        res[rel] = FileRecord(size=size, mtime=mtime, inode=inode, hash=h)
    return res
</file>

<file path="src/core/sync/types.py">
from dataclasses import dataclass, field
@dataclass(slots=True)
class FileRecord:
    size: int
    mtime: float
    inode: int | None
    hash: str
    def to_dict(self) -> dict:
        return {
            "size": self.size,
            "mtime": self.mtime,
            "inode": self.inode,
            "hash": self.hash,
        }
    @classmethod
    def from_dict(cls, d: dict) -> "FileRecord":
        return cls(
            size=int(d["size"]),
            mtime=float(d["mtime"]),
            inode=d.get("inode"),
            hash=str(d["hash"]),
        )
@dataclass
class DetectedChanges:
    added: list[str] = field(default_factory=list)
    modified: list[str] = field(default_factory=list)
    removed: list[str] = field(default_factory=list)
    @property
    def num_changes(self) -> int:
        return len(self.added) + len(self.modified) + len(self.removed)
    @property
    def to_add(self) -> list[str]:
        return self.added + self.modified
    @property
    def to_remove(self) -> list[str]:
        return self.modified + self.removed
</file>

<file path="src/core/sync/util.py">
DEFAULT_IGNORE_PATTERNS = [
    "node_modules/**",
    "bin/**",
    "venv/**",
    "dist/**",
    "build/**",
    "out/**",
    "target/**",
    "coverage/**",
    ".nyc_output/**",
    ".vscode/**",
    ".idea/**",
    "*.swp",
    "*.swo",
    ".git/**",
    ".svn/**",
    ".hg/**",
    ".cache/**",
    "__pycache__/**",
    ".pytest_cache/**",
    "logs/**",
    "tmp/**",
    "temp/**",
    "*.log",
    ".env",
    ".env.*",
    "*.local",
    "*.min.js",
    "*.min.css",
    "*.min.map",
    "*.bundle.js",
    "*.bundle.css",
    "*.chunk.js",
    "*.vendor.js",
    "*.polyfills.js",
    "*.runtime.js",
    "*.map",
    "node_modules",
    "venv",
    "bin",
    ".git",
    ".svn",
    ".hg",
    "build",
    "dist",
    "out",
    "target",
    ".vscode",
    ".idea",
    "__pycache__",
    ".pytest_cache",
    "coverage",
    ".nyc_output",
    "logs",
    "tmp",
    "temp",
]
</file>

<file path="src/core/__init__.py">
from .services import (
    EmbeddingConfig,
    EmbeddingService,
    ExplainerConfig,
    ExplainerService,
    IndexingService,
    SearchResult,
    SearchService,
)
from .splitters import TreeSitterSplitter
from .sync import FileSynchronizer
__all__ = [
    "IndexingService",
    "EmbeddingConfig",
    "ExplainerConfig",
    "ExplainerService",
    "EmbeddingService",
    "SearchService",
    "SearchResult",
    "TreeSitterSplitter",
    "FileSynchronizer",
]
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#   Usually these files are written by a python script from a template
#   before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
# Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
# uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
# poetry.lock
# poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
# pdm.lock
# pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
# pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# Redis
*.rdb
*.aof
*.pid

# RabbitMQ
mnesia/
rabbitmq/
rabbitmq-data/

# ActiveMQ
activemq-data/

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#   JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#   be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#   and can be added to the global gitignore or merged into this file.  For a more nuclear
#   option (not recommended) you can uncomment the following to ignore the entire idea folder.
# .idea/

# Abstra
#   Abstra is an AI-powered process automation framework.
#   Ignore directories containing user credentials, local state, and settings.
#   Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#   Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#   that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#   and can be added to the global gitignore or merged into this file. However, if you prefer, 
#   you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

# Streamlit
.streamlit/secrets.toml

snapshots/
</file>

<file path=".python-version">
3.13
</file>

<file path="pyproject.toml">
[project]
name = "core"
version = "0.5.2"
description = "Core module for semantic code search"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "fastembed>=0.7.3",
    "loguru>=0.7.3",
    "openai>=2.6.1",
    "qdrant-client>=1.15.1",
    "rerankers>=0.10.0",
    "tenacity>=9.1.2",
    "tree-sitter-language-pack>=0.10.0",
]


[tool.pyright]
exclude = ["**/node_modules", ".venv"]
include = ["./src/"]
venvPath = "."
venv = ".venv"

[tool.black]
line-length = 88
target-version = ["py313"]

[build-system]
requires = ["uv_build>=0.9.6,<0.10.0"]
build-backend = "uv_build"
</file>

<file path="README.md">
# Code Context Core

Python library for semantic code search and indexing. Provides vector-based code search with hybrid capabilities and incremental indexing.

## Installation

```bash
uv sync
```

## Quick Start

```python
import asyncio
from pathlib import Path
from qdrant_client import AsyncQdrantClient
from core import (
    EmbeddingService,
    SearchService,
    IndexingService,
    TreeSitterSplitter,
    FileSynchronizer
)

async def main():
    # Initialize services
    client = AsyncQdrantClient(url="http://localhost:6333")
    embedding_service = EmbeddingService("http://localhost:11434/v1", "ollama")
    synchronizer = FileSynchronizer("./snapshots")
    splitter = TreeSitterSplitter(chunk_size=2500, chunk_overlap=300)

    indexing_service = IndexingService(client, synchronizer)
    search_service = SearchService(
        client,
        embedding_service,
        embedding_model="vuongnguyen2212/CodeRankEmbed",
        explainer_embedding_model="hf.co/nomic-ai/nomic-embed-text-v1.5-GGUF:F16"
    )

    # Index a codebase
    await indexing_service.index(
        Path("./my-project"),
        splitter,
        embedding_config
    )

    # Search the codebase
    results = await search_service.search(
        Path("./my-project"),
        "function that handles user authentication",
        top_k=5
    )

    for result in results:
        print(f"{result.relative_path}:{result.start_line}-{result.end_line}")
        print(f"Language: {result.language}")
        print(f"Score: {result.score:.3f}")
        print(f"Content: {result.content[:200]}...")

asyncio.run(main())
```

## Core Components

### Services

- **IndexingService**: Orchestrates file processing and vector storage
- **SearchService**: Handles semantic search queries with hybrid capabilities
- **EmbeddingService**: Manages embedding generation via OpenAI-compatible APIs
- **ExplainerService**: Provides code explanations using LLMs
- **FileSynchronizer**: Handles incremental updates with change detection

### Splitters

- **TreeSitterSplitter**: Language-aware code splitting using tree-sitter parsers
- **BaseSplitter**: Abstract base for custom splitters

### Utilities

- **CollectionName**: Generates unique collection names per project
- **Embedding**: Type definitions for embedding vectors

## API

### Main Classes

#### IndexingService
```python
async def index(
    self,
    path: Path,
    splitter: TreeSitterSplitter,
    embedding_config: EmbeddingConfig,
    force_reindex: bool = False
) -> None
```

#### SearchService
```python
async def search(
    self,
    path: Path,
    query: str,
    top_k: int = 5,
    threshold: float = 0.0
) -> list[SearchResult]
```

#### EmbeddingService
```python
async def generate_embedding(self, query: str, model: str) -> Embedding
async def generate_embeddings(
    self,
    queries: list[str],
    model: str,
    batch_size: int = 32
) -> list[Embedding]
```

### Data Models

```python
@dataclass
class SearchResult:
    content: str           # Code chunk content
    relative_path: str     # Relative file path
    start_line: int        # Start line number
    end_line: int          # End line number
    language: str          # Programming language
    score: float           # Similarity score (0-1)
    explanation: str | None = None  # Code explanation if enabled

@dataclass
class EmbeddingConfig:
    service: EmbeddingService
    model: str
    size: int | None = None
```

## Supported Languages

JavaScript, TypeScript, Python, Java, C/C++, C#, Go, Rust, Scala, PHP, Ruby, Swift, Kotlin

Language detection is automatic based on file extensions using tree-sitter parsers.

## Features

- **Hybrid Search**: Combines semantic similarity and keyword matching
- **Incremental Updates**: Merkle tree-based change detection
- **Language-Aware Splitting**: Context-preserving code chunking
- **Async Operations**: Full async/await support
</file>

</files>
